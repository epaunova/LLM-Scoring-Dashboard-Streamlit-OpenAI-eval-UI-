# ðŸ§  LLM Scoring Dashboard

Streamlit-based interactive dashboard to evaluate LLM outputs on key qualitative metrics:

- Factuality
- Clarity
- Style

## ðŸ’¡ Features

- Upload CSV with Prompt + Output
- Manual scoring using dropdowns
- Auto-evaluation extension ready (mock scoring or OpenAI integration)
- Saves scores to CSV for later analysis

## ðŸ›  How to Run

```bash
pip install streamlit pandas
streamlit run app.py
```

## ðŸ“‚ Structure

- `app.py` â€“ main Streamlit app
- `data/mock_outputs.csv` â€“ example inputs
- `prompts/`, `scoring/`, `utils/` â€“ placeholders for extensions
