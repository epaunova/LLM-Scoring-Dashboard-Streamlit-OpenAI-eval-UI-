# ğŸ§  LLM Scoring Dashboard

This is a Streamlit-based dashboard that allows manual evaluation of LLM outputs across key quality dimensions:

- âœ… Factuality  
- âœ¨ Clarity  
- ğŸ–‹ Style

Itâ€™s designed as a lightweight interactive tool for PMs, researchers, and developers working with LLM-generated content â€” enabling fast evaluation loops and structured feedback.

---

## ğŸ”§ Features

- Upload a CSV with prompt + model outputs
- Score each output using dropdowns per category
- View data in real time, then export scored results
- Ready for extension with GPT-based auto-eval or OpenAI API

---

## ğŸ“Š Example Input Format

`data/mock_outputs.csv`

```csv
Prompt,Output
"Summarize WW2","World War II began in 1939 and ended in 1945..."
"Explain photosynthesis","Photosynthesis is the process by which green plants..."
"What is blockchain?","Blockchain is a distributed ledger technology..."
ğŸš€ How to Run
bash
Copy
Edit
pip install streamlit pandas
streamlit run app.py
Then upload your mock_outputs.csv or a custom file and begin scoring interactively.

ğŸ“ Project Structure
bash
Copy
Edit
llm-scoring-dashboard/
â”œâ”€â”€ app.py                          # Main Streamlit app
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mock_outputs.csv           # Example input file
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ example_prompt.txt         # Prompt template placeholder
â”œâ”€â”€ scoring/
â”‚   â””â”€â”€ eval_logic_notes.md        # Future scoring extensions
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ token_counter_placeholder.py  # Utility stub
â””â”€â”€ README.md
ğŸ”— Related Projects
âœï¸ Prompt Efficiency Sandbox

ğŸ“Š LLM Eval Playground

ğŸ§ª LLM Evaluation Toolkit

Together, these projects cover the full lifecycle:
Prompt Design â†’ Output Evaluation â†’ Model Comparison â†’ Human Feedback Loop

ğŸ‘¤ Author
Eva Paunova
ğŸ”— LinkedIn:( https://www.linkedin.com/in/eva-hristova-paunova-a194b3210/)
ğŸ“‚ Portfolio
